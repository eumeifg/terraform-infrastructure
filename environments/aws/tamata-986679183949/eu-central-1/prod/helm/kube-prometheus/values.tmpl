fullnameOverride: "prometheus"
defaultRules:
  rules:
    general: false
    etcd: false
    nodeExporterAlerting: false
    nodeExporterRecording: false
    kubernetesApps: false
kubeControllerManager:
  enabled: false
kubeEtcd:
  enabled: false
kubeScheduler:
  enabled: false
prometheus:
  prometheusSpec:
    externalLabels:
      cluster: ${cluster_name}
    serviceMonitorSelectorNilUsesHelmValues: false
    prometheusExternalLabelName: cluster
    retention: 45d
    retentionSize: "50GB"
    walCompression: true
    logLevel: warn
    additionalScrapeConfigs: []
    storageSpec:
      volumeClaimTemplate:
        spec:
          storageClassName: ebs-sc
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi
    additionalScrapeConfigs:
      - job_name: 'kubernetes-pods'
        scrape_interval: 30s
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]
            regex: "redis"
            action: drop
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name



    tolerations:
      - key: "app.kubernetes.io/name"
        operator: "Equal"
        value: "karpenter"
        effect: "NoSchedule"

additionalPrometheusRulesMap:
  - name: prometheus
    groups:
      - name: prometheus
        rules:
          - alert: PrometheusAlertmanagerE2eDeadManSwitch
            expr: vector(1)
            for: 0m
            labels:
              severity: critical
              service: dead-man-switch
            annotations:
              summary: Prometheus AlertManager E2E dead man switch (instance {{ $labels.instance }})
              description: "Prometheus DeadManSwitch is an always-firing alert. It's used as an end-to-end test of Prometheus through the Alertmanager."
  - name: host-and-hardware
    groups:
      - name: node-exporter
        rules:
          - alert: HostOutOfMemory
            expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: Host out of memory (instance {{ $labels.instance }})
              description: "Node memory is filling up (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: HostMemoryUnderMemoryPressure
            expr: rate(node_vmstat_pgmajfault[1m]) > 1000
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host memory under memory pressure (instance {{ $labels.instance }})
              description: "The node is under heavy memory pressure. High rate of major page faults\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: HostUnusualNetworkThroughputIn
            expr: sum by (instance) (rate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: Host unusual network throughput in (instance {{ $labels.instance }})
              description: "Host network interfaces are probably receiving too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: HostUnusualDiskReadRate
            expr: sum by (instance) (rate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 50
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: Host unusual disk read rate (instance {{ $labels.instance }})
              description: "Disk is probably reading too much data (> 50 MB/s)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: HostUnusualDiskWriteRate
            expr: sum by (instance) (rate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 50
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host unusual disk write rate (instance {{ $labels.instance }})
              description: "Disk is probably writing too much data (> 50 MB/s)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: HostOutOfDiskSpace
            expr: (node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) node_filesystem_readonly == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: Host out of disk space (instance {{ $labels.instance }})
              description: "Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: HostDiskWillFillIn24Hours
            expr: (node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) predict_linear(node_filesystem_avail_bytes{fstype!~"tmpfs"}[1h], 24 * 3600) < 0 and ON (instance, device, mountpoint) node_filesystem_readonly == 0
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host disk will fill in 24 hours (instance {{ $labels.instance }})
              description: "Filesystem is predicted to run out of space within the next 24 hours at current write rate\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: HostOutOfInodes
            expr: node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint="/rootfs"} * 100 < 10 and ON (instance, device, mountpoint) node_filesystem_readonly{mountpoint="/rootfs"} == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: Host out of inodes (instance {{ $labels.instance }})
              description: "Disk is almost running out of available inodes (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: HostInodesWillFillIn24Hours
            expr: node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint="/rootfs"} * 100 < 10 and predict_linear(node_filesystem_files_free{mountpoint="/rootfs"}[1h], 24 * 3600) < 0 and ON (instance, device, mountpoint) node_filesystem_readonly{mountpoint="/rootfs"} == 0
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host inodes will fill in 24 hours (instance {{ $labels.instance }})
              description: "Filesystem is predicted to run out of inodes within the next 24 hours at current write rate\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: HostHighCpuLoad
            expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[2m])) * 100) > 80
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Host high CPU load (instance {{ $labels.instance }})
              description: "CPU load is > 80%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: HostCpuStealNoisyNeighbor
            expr: avg by(instance) (rate(node_cpu_seconds_total{mode="steal"}[5m])) * 100 > 10
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Host CPU steal noisy neighbor (instance {{ $labels.instance }})
              description: "CPU steal is > 10%. A noisy neighbor is killing VM performances or a spot instance may be out of credit.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: HostOomKillDetected
            expr: increase(node_vmstat_oom_kill[1m]) > 0
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Host OOM kill detected (instance {{ $labels.instance }})
              description: "OOM kill detected\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
  - name: kubernetes
    groups:
      - name: kube-state-metrics
        rules:
          - alert: KubernetesNodeReady
            expr: kube_node_status_condition{condition="Ready",status="true"} == 0
            for: 10m
            labels:
              severity: critical
            annotations:
              summary: Kubernetes Node ready (instance {{ $labels.instance }})
              description: "Node {{ $labels.node }} has been unready for a long time\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesContainerOomKiller
            expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Kubernetes container oom killer (instance {{ $labels.instance }})
              description: "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesPersistentvolumeclaimPending
            expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Kubernetes PersistentVolumeClaim pending (instance {{ $labels.instance }})
              description: "PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesPersistentvolumeError
            expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending", job="kube-state-metrics"} > 0
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Kubernetes PersistentVolume error (instance {{ $labels.instance }})
              description: "Persistent volume is in bad state\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesStatefulsetDown
            expr: (kube_statefulset_status_replicas_ready / kube_statefulset_status_replicas_current) != 1
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: Kubernetes StatefulSet down (instance {{ $labels.instance }})
              description: "A StatefulSet went down\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesPodNotHealthy
            expr: min_over_time(sum by (namespace, pod) (kube_pod_status_phase{phase=~"Unknown|Failed"})[15m:1m]) > 0
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Kubernetes Pod not healthy (instance {{ $labels.instance }})
              description: "Pod has been in a non-ready state for longer than 15 minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesPodCrashLooping
            expr: increase(kube_pod_container_status_restarts_total[1m]) > 3
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: Kubernetes pod crash looping (instance {{ $labels.instance }})
              description: "Pod {{ $labels.pod }} is crash looping\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesStatefulsetUpdateNotRolledOut
            expr: max without (revision) (kube_statefulset_status_current_revision unless kube_statefulset_status_update_revision) * (kube_statefulset_replicas != kube_statefulset_status_replicas_updated)
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: Kubernetes StatefulSet update not rolled out (instance {{ $labels.instance }})
              description: "StatefulSet update has not been rolled out.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesDaemonsetRolloutStuck
            expr: kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled * 100 < 100 or kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: Kubernetes DaemonSet rollout stuck (instance {{ $labels.instance }})
              description: "Some Pods of DaemonSet are not scheduled or not ready\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesApiServerErrors
            expr: sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[1m])) / sum(rate(apiserver_request_count{job="apiserver"}[1m])) * 100 > 3
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: Kubernetes API server errors (instance {{ $labels.instance }})
              description: "Kubernetes API server is experiencing high error rate\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
  - name: redis
    groups:
      - name: redis
        rules:
          - alert: RedisDown
            expr: redis_up == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: Redis; instance {{ $labels.instance }} down
              description: Redis; instance {{ $labels.instance }} is down
          - alert: RedisKeyEviction
            expr: |
              increase(redis_evicted_keys_total{}[5m]) > 0
            for: 1s
            labels:
              severity: error
            annotations:
              summary: Redis; instance {{ $labels.instance }} has evicted keys
              description: |
                Redis; instance {{ $labels.instance }} has evicted {{ $value }} keys in the last 5 minutes.
  - name: aws-rds
    groups:
      - name: aws-rds
        rules:
          - alert: RDSHighCPUUsage
            expr: avg by (dbinstance_identifier)(aws_rds_cpuutilization_average offset 10m) > 80
            for: 10m
            labels:
              severity: critical
            annotations:
              summary: AWS RDS instance {{ $labels.dbinstance_identifier }} has high CPU usage
              description: |
                AWS RDS instance {{ $labels.dbinstance_identifier }} has more than %80 CPU utilization for 10m.
                The CPU usage is {{ $value }}%.
          - alert: RDSNotEnoughFreeMemory
            expr: avg by (dbinstance_identifier)(aws_rds_freeable_memory_average offset 10m) < 512*1024*1024 # 512 MiB
            for: 10m
            labels:
              severity: critical
            annotations:
              summary: AWS RDS instance {{ $labels.dbinstance_identifier }} hasn't enough free memory
              description: |
                AWS RDS instance {{ $labels.dbinstance_identifier }} has less than 500 MiB free memory.
                Current available memory is {{ $value }} bytes.
          - alert: RDSHighDatabaseConnectionCount
            expr: avg by (dbinstance_identifier)(aws_rds_database_connections_average offset 10m) / avg by (dbinstance_identifier)(aws_rds_database_connections_average offset 70m) > 10
            for: 3m
            labels:
              severity: critical
            annotations:
              summary: AWS RDS instance {{ $labels.dbinstance_identifier }} has high connection count
              description: |
                AWS RDS instance {{ $labels.dbinstance_identifier }}'s number of connections has increased 10 times in 3 minutes.
          - alert: RDSStorageIsFillingUp
            expr: avg by (dbinstance_identifier)(aws_rds_free_storage_space_average offset 10m) < 1024*1024*1024 # 1GiB
            for: 30m
            labels:
              severity: critical
            annotations:
              summary: AWS RDS instance {{ $labels.dbinstance_identifier }} storage is filling up.
              description: |
                Less than 1 GiB storage available for AWS RDS instance {{ $labels.dbinstance_identifier }}.
                Current free storage is {{ $value }} bytes.
  - name: aws-alb
    groups:
      - name: aws-alb
        rules:
          - alert: HighALB5xxError
            expr: aws_applicationelb_httpcode_target_5_xx_count_sum offset 10m / aws_applicationelb_request_count_sum offset 10m * 100 > 5
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: AWS ALB returning too much 5xx error from {{ $labels.target_group }} target group.
              description: |
                AWS ALB 5xx error count rate more than 5% of total requests.
                The error count rate is {{ $value }}%.
  - name: blackbox-exporter
    groups:
      - name: blackbox.rules
        rules:
          - alert: Endpoint down
            expr: probe_success != 1
            for: 1m
            labels:
              severity: warning
            annotations:
              summary: "{{ $labels.instance }} is down"
          - alert: Endpoint down
            expr: probe_success != 1
            for: 5m
            labels:
              severity: critical
            annotations:
              summary: "{{ $labels.instance }} is down!"
          - alert: Probe duration high
            expr: probe_duration_seconds > 1 # 1000 milliseconds
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: "Probing {{ $labels.instance }} took {{ $value }} seconds"
          - alert: Probe duration high
            expr: probe_duration_seconds > 2 # 2000 milliseconds
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "Probing {{ $labels.instance }} took {{ $value }} seconds!"
          - alert: Probe DNS lookup duration high
            expr: probe_dns_lookup_time_seconds > .1 # 100 milliseconds
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: "{{ $labels.instance }} DNS lookup took {{ $value }}"
          - alert: Probe DNS lookup duration high
            expr: probe_dns_lookup_time_seconds > .2 # 200 milliseconds
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "{{ $labels.instance }} DNS lookup took {{ $value }}!"

alertmanager:
  config:
    global:
      resolve_timeout: 5m
    route:
      group_by: ['job', 'alertname', 'cluster', 'service']
      repeat_interval: 3h
      receiver: webex
      routes:
      - receiver: healthchecks.io
        matchers:
        - service="dead-man-switch"
        group_wait: 0s
        group_interval: 1m
        repeat_interval: 60s
      - receiver: blackhole
        matchers:
        - severity="info"
      - receiver: opsgenie
        matchers:
        - severity="critical"
    receivers:
    - name: blackhole
    - name: healthchecks.io
      webhook_configs:
      - url: ${healthchecks_io_url}
        send_resolved: true
    - name: webex
      webhook_configs:
      - url: ${webex_webhook_url}
        send_resolved: true
    - name: opsgenie
      opsgenie_configs:
      - api_key: ${opsgenie_api_key}
        priority: '{{ if eq .CommonLabels.severity "critical" }}P1{{ else if eq .CommonLabels.severity "warning" }}P2{{ else if eq .CommonLabels.severity "info" }}P3{{ else }}P4{{ end }}'
        message: '{{ .CommonLabels.cluster }}: {{ .CommonLabels.alertname }} in {{ if ne .CommonLabels.exported_namespace "" }}{{ .CommonLabels.exported_namespace }}{{ else }}{{ .CommonLabels.namespace }}{{ end }}'
        description: |-
          {{ if gt (len .Alerts.Firing) 0 -}}
          Alerts Firing:
          {{ range .Alerts.Firing }}
            - Message: {{ .Annotations.summary }}
            - Description: {{ .Annotations.description }}
              Labels:
          {{ range .Labels.SortedPairs }}   - {{ .Name }} = {{ .Value }}
          {{ end }}   Annotations:
          Source: {{ .GeneratorURL }}
          {{ end }}
          {{- end }}
          {{ if gt (len .Alerts.Resolved) 0 -}}
          Alerts Resolved:
          {{ range .Alerts.Resolved }}
            - Message: {{ .Annotations.summary }}
            - Description: {{ .Annotations.description }}
            Labels:
          {{ range .Labels.SortedPairs }}   - {{ .Name }} = {{ .Value }}
          {{ end }}   Annotations:
          Source: {{ .GeneratorURL }}
          {{ end }}
          {{- end }}
        details:
          cluster: ${cluster_name}
        send_resolved: true
    templates:
    - '/etc/alertmanager/config/*.tmpl'

grafana:
  serviceAccount:
    annotations:
      eks.amazonaws.com/role-arn: ${grafana_role}
  persistence:
    type: pvc
    enabled: true
    storageClassName: ebs-sc

  tolerations:
    - key: group
      operator: Equal
      value: main
      effect: NoSchedule

  ingress:
    enabled: true
    ingressClassName: alb-class
    annotations:
      alb.ingress.kubernetes.io/group.name: grafana-monitoring
      alb.ingress.kubernetes.io/listen-ports: '[{"HTTP":80}, {"HTTPS":443}]'
      alb.ingress.kubernetes.io/ssl-redirect: '443'
      alb.ingress.kubernetes.io/scheme: internet-facing
      alb.ingress.kubernetes.io/group.name: tamata-prod
      alb.ingress.kubernetes.io/subnets: subnet-077b84f55c79187cd, subnet-05976ca81e3e56146, subnet-07c293469f612eb35
      alb.ingress.kubernetes.io/target-type: ip
      alb.ingress.kubernetes.io/tags: Environment=production
      alb.ingress.kubernetes.io/load-balancer-attributes: routing.http.drop_invalid_header_fields.enabled=true, deletion_protection.enabled=true, access_logs.s3.enabled=true, access_logs.s3.bucket=tamata-alb-logging, access_logs.s3.prefix=ingress-prod
      alb.ingress.kubernetes.io/healthcheck-port: '3000'
      alb.ingress.kubernetes.io/success-codes: 200,302
      external-dns.alpha.kubernetes.io/hostname: g-2a6t55fk8kvmex6.tamata.com
    hosts:
      - g-2a6t55fk8kvmex6.tamata.com
    path: /

  grafana.ini:
    users:
      viewers_can_edit: false
    auth:
      disable_login_form: false
      disable_signout_menu: false
    auth.anonymous:
      enabled: false
      org_role: Viewer
    server:
      root_url: https://g-2a6t55fk8kvmex6.tamata.com
    auth.generic_oauth:
      name: BitBucket
      enabled: true
      allow_sign_up: true
      client_id: ${client_id}
      client_secret: ${client_secret}
      scopes: account email
      auth_url: https://bitbucket.org/site/oauth2/authorize
      token_url: https://bitbucket.org/site/oauth2/access_token
      api_url: https://api.bitbucket.org/2.0/user
      teams_url: https://api.bitbucket.org/2.0/user/permissions/workspaces
      team_ids_attribute_path: values[*].workspace.slug
      team_ids: creativeadvtech

prometheus-node-exporter:
  resources:
    limits:
      memory: 2048Mi
    requests:
      cpu: 100m
      memory: 128Mi
