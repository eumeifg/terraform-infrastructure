fullnameOverride: "prometheus"
defaultRules:
  rules:
    general: false
    etcd: false
    kubeSchedulerAlerting: false
    kubeSchedulerRecording: false
    nodeExporterAlerting: false
    kubernetesApps: false
    kubeProxy: false
kubeControllerManager:
  enabled: false
kubeProxy:
  enabled: false
kubeEtcd:
  enabled: false
kubeScheduler:
  enabled: false
prometheus:
  prometheusSpec:
    externalLabels:
      cluster: ${cluster_name}
    serviceMonitorSelectorNilUsesHelmValues: false
    prometheusExternalLabelName: cluster
    retention: 45d
    retentionSize: "15GB"
    walCompression: true
    logLevel: warn
    resources:
      limits:
        memory: 4Gi
      requests:
        cpu: 1
        memory: 4Gi
    storageSpec:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 16Gi
additionalPrometheusRulesMap:
  rule-name:
    groups:
      - name: prometheus
        rules:
          - alert: PrometheusAlertmanagerE2eDeadManSwitch
            expr: vector(1)
            for: 0m
            labels:
              severity: critical
              service: dead-man-switch
            annotations:
              summary: Prometheus AlertManager E2E dead man switch (instance {{ $labels.instance }})
              description: "Prometheus DeadManSwitch is an always-firing alert. It's used as an end-to-end test of Prometheus through the Alertmanager."
      - name: node-exporter
        rules:
          - alert: HostOutOfMemory
            expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: Host out of memory (instance {{ $labels.instance }})
              description: "Node memory is filling up (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: HostMemoryUnderMemoryPressure
            expr: rate(node_vmstat_pgmajfault[1m]) > 1000
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host memory under memory pressure (instance {{ $labels.instance }})
              description: "The node is under heavy memory pressure. High rate of major page faults\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: HostUnusualNetworkThroughputIn
            expr: sum by (instance) (rate(node_network_receive_bytes_total[2m])) / 1024 / 1024 > 100
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: Host unusual network throughput in (instance {{ $labels.instance }})
              description: "Host network interfaces are probably receiving too much data (> 100 MB/s)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: HostUnusualDiskReadRate
            expr: sum by (instance) (rate(node_disk_read_bytes_total[2m])) / 1024 / 1024 > 50
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: Host unusual disk read rate (instance {{ $labels.instance }})
              description: "Disk is probably reading too much data (> 50 MB/s)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: HostUnusualDiskWriteRate
            expr: sum by (instance) (rate(node_disk_written_bytes_total[2m])) / 1024 / 1024 > 50
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host unusual disk write rate (instance {{ $labels.instance }})
              description: "Disk is probably writing too much data (> 50 MB/s)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: HostOutOfDiskSpace
            expr: (node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) node_filesystem_readonly == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: Host out of disk space (instance {{ $labels.instance }})
              description: "Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: HostDiskWillFillIn24Hours
            expr: (node_filesystem_avail_bytes * 100) / node_filesystem_size_bytes < 10 and ON (instance, device, mountpoint) predict_linear(node_filesystem_avail_bytes{fstype!~"tmpfs"}[1h], 24 * 3600) < 0 and ON (instance, device, mountpoint) node_filesystem_readonly == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: Host disk will fill in 24 hours (instance {{ $labels.instance }})
              description: "Filesystem is predicted to run out of space within the next 24 hours at current write rate\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: HostOutOfInodes
            expr: node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint="/rootfs"} * 100 < 10 and ON (instance, device, mountpoint) node_filesystem_readonly{mountpoint="/rootfs"} == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: Host out of inodes (instance {{ $labels.instance }})
              description: "Disk is almost running out of available inodes (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: HostInodesWillFillIn24Hours
            expr: node_filesystem_files_free{mountpoint ="/rootfs"} / node_filesystem_files{mountpoint="/rootfs"} * 100 < 10 and predict_linear(node_filesystem_files_free{mountpoint="/rootfs"}[1h], 24 * 3600) < 0 and ON (instance, device, mountpoint) node_filesystem_readonly{mountpoint="/rootfs"} == 0
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: Host inodes will fill in 24 hours (instance {{ $labels.instance }})
              description: "Filesystem is predicted to run out of inodes within the next 24 hours at current write rate\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: HostHighCpuLoad
            expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[2m])) * 100) > 80
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Host high CPU load (instance {{ $labels.instance }})
              description: "CPU load is > 80%\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: HostCpuStealNoisyNeighbor
            expr: avg by(instance) (rate(node_cpu_seconds_total{mode="steal"}[5m])) * 100 > 10
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Host CPU steal noisy neighbor (instance {{ $labels.instance }})
              description: "CPU steal is > 10%. A noisy neighbor is killing VM performances or a spot instance may be out of credit.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: HostOomKillDetected
            expr: increase(node_vmstat_oom_kill[1m]) > 0
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Host OOM kill detected (instance {{ $labels.instance }})
              description: "OOM kill detected\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
      - name: kube-state-metrics
        rules:
          - alert: KubernetesNodeReady
            expr: kube_node_status_condition{condition="Ready",status="true"} == 0
            for: 10m
            labels:
              severity: critical
            annotations:
              summary: Kubernetes Node ready (instance {{ $labels.instance }})
              description: "Node {{ $labels.node }} has been unready for a long time\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesContainerOomKiller
            expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Kubernetes container oom killer (instance {{ $labels.instance }})
              description: "Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesJobFailed
            expr: kube_job_status_failed > 0
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Kubernetes Job failed (instance {{ $labels.instance }})
              description: "Job {{$labels.namespace}}/{{$labels.exported_job}} failed to complete\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesCronjobSuspended
            expr: kube_cronjob_spec_suspend != 0
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Kubernetes CronJob suspended (instance {{ $labels.instance }})
              description: "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is suspended\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesPersistentvolumeclaimPending
            expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Kubernetes PersistentVolumeClaim pending (instance {{ $labels.instance }})
              description: "PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesPersistentvolumeError
            expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending", job="kube-state-metrics"} > 0
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Kubernetes PersistentVolume error (instance {{ $labels.instance }})
              description: "Persistent volume is in bad state\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesStatefulsetDown
            expr: (kube_statefulset_status_replicas_ready / kube_statefulset_status_replicas_current) != 1
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: Kubernetes StatefulSet down (instance {{ $labels.instance }})
              description: "A StatefulSet went down\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesPodNotHealthy
            expr: min_over_time(sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"})[15m:1m]) > 0
            for: 3m
            labels:
              severity: critical
            annotations:
              summary: Kubernetes Pod not healthy (instance {{ $labels.instance }})
              description: "Pod has been in a non-ready state for longer than 3 minutes.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesPodCrashLooping
            expr: increase(kube_pod_container_status_restarts_total[1m]) > 3
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: Kubernetes pod crash looping (instance {{ $labels.instance }})
              description: "Pod {{ $labels.pod }} is crash looping\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesStatefulsetUpdateNotRolledOut
            expr: max without (revision) (kube_statefulset_status_current_revision unless kube_statefulset_status_update_revision) * (kube_statefulset_replicas != kube_statefulset_status_replicas_updated)
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: Kubernetes StatefulSet update not rolled out (instance {{ $labels.instance }})
              description: "StatefulSet update has not been rolled out.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesDaemonsetRolloutStuck
            expr: kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled * 100 < 100 or kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: Kubernetes DaemonSet rollout stuck (instance {{ $labels.instance }})
              description: "Some Pods of DaemonSet are not scheduled or not ready\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
          - alert: KubernetesApiServerErrors
            expr: sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[1m])) / sum(rate(apiserver_request_count{job="apiserver"}[1m])) * 100 > 3
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: Kubernetes API server errors (instance {{ $labels.instance }})
              description: "Kubernetes API server is experiencing high error rate\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
alertmanager:
  config:
    global:
      resolve_timeout: 5m
    route:
      group_by: ['job', 'alertname', 'cluster', 'service']
      repeat_interval: 3h
      receiver: blackhole
      routes:
      - receiver: healthchecks.io
        matchers:
        - service="dead-man-switch"
        group_wait: 0s
        group_interval: 1m
        repeat_interval: 60s
      - receiver: blackhole
        matchers:
        - severity="info"
    receivers:
    - name: blackhole
    - name: healthchecks.io
      webhook_configs:
      - url: ${healthchecks_io_url}
        send_resolved: false
    templates:
    - '/etc/alertmanager/config/*.tmpl'
  alertmanagerSpec:
    logLevel: warn
    storage:
      volumeClaimTemplate:
        spec:
          resources:
            requests:
              storage: 1Gi
    resources:
      limits:
        memory: 64Mi
      requests:
        cpu: 20m
        memory: 64Mi
grafana:
  defaultDashboardsTimezone: browser
  adminPassword: ${grafana_admin_password}
  serviceAccount:
    annotations:
      eks.amazonaws.com/role-arn: ${grafana_role}
  persistence:
    enabled: true
    size: 1Gi
  deploymentStrategy:
    type: Recreate
  ingress:
    enabled: true
    ingressClassName: alb
    annotations:
      alb.ingress.kubernetes.io/listen-ports: '[{"HTTP":80}, {"HTTPS":443}]'
      alb.ingress.kubernetes.io/ssl-redirect: '443'
      alb.ingress.kubernetes.io/scheme: internet-facing
      alb.ingress.kubernetes.io/group.name: tasleem-staging
      alb.ingress.kubernetes.io/subnets: subnet-0c58c5c661490e43a, subnet-0817c7f514c66f986
      alb.ingress.kubernetes.io/target-type: ip
      alb.ingress.kubernetes.io/tags: Environment=staging
      alb.ingress.kubernetes.io/load-balancer-attributes: deletion_protection.enabled=true
      alb.ingress.kubernetes.io/healthcheck-port: '3000'
      alb.ingress.kubernetes.io/success-codes: 200,302
      external-dns.alpha.kubernetes.io/hostname: grafana.tasleem.creativeadvtech.ml
    hosts:
      - grafana.tasleem.creativeadvtech.ml
    path: /
  grafana.ini:
    users:
      viewers_can_edit: false
    auth:
      disable_login_form: false
      disable_signout_menu: false
    auth.anonymous:
      enabled: false
      org_role: Viewer
    server:
      root_url: https://grafana.tasleem.creativeadvtech.ml
    auth.generic_oauth:
      name: BitBucket
      enabled: true
      allow_sign_up: true
      client_id: ${client_id}
      client_secret: ${client_secret}
      scopes: account email
      auth_url: https://bitbucket.org/site/oauth2/authorize
      token_url: https://bitbucket.org/site/oauth2/access_token
      api_url: https://api.bitbucket.org/2.0/user
      teams_url: https://api.bitbucket.org/2.0/user/permissions/workspaces
      team_ids_attribute_path: values[*].workspace.slug
      team_ids: creativeadvtech
prometheusOperator:
  resources:
    limits:
      memory: 256Mi
    requests:
      cpu: 50m
      memory: 256Mi
prometheus-node-exporter:
  resources:
    limits:
      memory: 32Mi
    requests:
      cpu: 10m
      memory: 32Mi
